"""
é›†æˆå„ªåŒ–çš„RAGç³»çµ±
å°‡æ‰€æœ‰å„ªåŒ–çµ„ä»¶æ•´åˆåˆ°ç¾æœ‰RAGç³»çµ±ä¸­
"""

import asyncio
import time
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass

from config.settings import get_settings
from core.models.ollama_client import get_ollama_client
from core.storage.vector_store import get_vector_store_manager
from core.rag.document_processing import DocumentChunk
from core.memory.context_manager import ContextManager

# å°å…¥å„ªåŒ–çµ„ä»¶
from .cache_manager import get_cache_manager, cached
from .batch_processor import create_batch_processor, BatchConfig
from .parallel_executor import ParallelExecutor, ExecutorConfig, Task
from .recovery_manager import get_retry_executor, smart_retry
from .resource_monitor import get_resource_monitor, monitor_performance
from .enhanced_retrieval import EnhancedRetriever, SearchQuery as OptimizedSearchQuery, SearchResult


@dataclass
class OptimizedRetrievalResult:
    """å„ªåŒ–çš„æª¢ç´¢çµæœ"""
    answer: str
    sources: List[DocumentChunk]
    confidence: float
    retrieval_time: float
    generation_time: float
    total_time: float
    metadata: Dict[str, Any]
    
    # æ–°å¢å„ªåŒ–ç›¸é—œæŒ‡æ¨™
    cache_hit: bool = False
    retry_count: int = 0
    batch_processed: bool = False
    memory_usage_mb: float = 0.0
    optimization_stats: Dict[str, Any] = None


class OptimizedRAGSystem:
    """å„ªåŒ–çš„RAGç³»çµ±"""
    
    def __init__(self, user_id: str = "default"):
        self.settings = get_settings()
        self.ollama_client = get_ollama_client()
        self.vector_store_manager = get_vector_store_manager()
        self.context_manager = ContextManager(user_id)
        self.user_id = user_id
        
        # å„ªåŒ–çµ„ä»¶
        self.cache = get_cache_manager()
        self.monitor = get_resource_monitor()
        self.retry_executor = get_retry_executor()
        self.enhanced_retriever = EnhancedRetriever()
        
        # æ‰¹è™•ç†é…ç½®
        self.batch_processor = create_batch_processor(
            processing_mode="async",
            batch_size=32,
            adaptive=False  # ä½¿ç”¨æ™®é€šçš„BatchProcessorè€Œä¸æ˜¯AdaptiveBatchProcessor
        )
        
        # ä¸¦è¡ŒåŸ·è¡Œå™¨
        self.parallel_executor = ParallelExecutor(ExecutorConfig(
            max_workers=4,
            timeout_seconds=60
        ))
        
        # å•Ÿå‹•ç›£æ§
        if not getattr(self.monitor, '_monitoring', False):
            self.monitor.start_monitoring()
    
    @monitor_performance("rag_query")
    @smart_retry(max_retries=3, base_delay=1.0)
    async def query(self, 
                   question: str,
                   filters: Optional[Dict[str, Any]] = None,
                   max_results: int = 5,
                   similarity_threshold: float = 0.7,
                   include_memory_context: bool = True,
                   use_batch_processing: bool = False,
                   search_strategy: str = "hybrid") -> OptimizedRetrievalResult:
        """å„ªåŒ–çš„æŸ¥è©¢æ–¹æ³•"""
        
        start_time = time.time()
        optimization_stats = {
            "cache_operations": 0,
            "retry_attempts": 0,
            "batch_size": 1,
            "parallel_tasks": 0
        }
        
        try:
            # ç²å–è¨˜æ†¶ä¸Šä¸‹æ–‡
            enhanced_question = question
            memory_context = ""
            if include_memory_context:
                memory_context = await self.context_manager.get_smart_context(question)
                if memory_context:
                    enhanced_question = f"{question}\n\nç›¸é—œä¸Šä¸‹æ–‡ï¼š\n{memory_context}"
            
            # æª¢æŸ¥ç·©å­˜
            cache_key = f"rag_query:{hash(enhanced_question + str(filters))}"
            cached_result = await self.cache.get(cache_key)
            optimization_stats["cache_operations"] += 1
            
            if cached_result:
                cached_result.cache_hit = True
                cached_result.total_time = time.time() - start_time
                return cached_result
            
            # åŸ·è¡Œæª¢ç´¢
            retrieval_start = time.time()
            
            # å‰µå»ºå„ªåŒ–çš„æœç´¢æŸ¥è©¢
            search_query = OptimizedSearchQuery(
                text=enhanced_question,
                limit=max_results * 2,  # ç²å–æ›´å¤šå€™é¸é€²è¡Œé‡æ’åº
                similarity_threshold=similarity_threshold,
                filters=filters or {},
                search_strategy=search_strategy,
                rerank=True,
                use_cache=True
            )
            
            # åŸ·è¡Œå¢å¼·æª¢ç´¢
            search_results, retrieval_metrics = await self.enhanced_retriever.search(search_query)
            
            retrieval_time = time.time() - retrieval_start
            
            # è½‰æ›æœç´¢çµæœç‚ºDocumentChunkæ ¼å¼
            sources = []
            for result in search_results[:max_results]:
                chunk = DocumentChunk(
                    id=result.id,
                    content=result.content,
                    chunk_index=result.chunk_index,
                    source_file=result.source,
                    metadata=result.metadata
                )
                sources.append(chunk)
            
            # ç”Ÿæˆç­”æ¡ˆ
            generation_start = time.time()
            
            if use_batch_processing and len(sources) > 5:
                # ä½¿ç”¨æ‰¹è™•ç†ç”Ÿæˆç­”æ¡ˆ
                answer = await self._batch_generate_answer(enhanced_question, sources)
                optimization_stats["batch_size"] = len(sources)
            else:
                # å–®æ¬¡ç”Ÿæˆç­”æ¡ˆ
                answer = await self._generate_answer(enhanced_question, sources)
            
            generation_time = time.time() - generation_start
            
            # è¨ˆç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(search_results, answer)
            
            # ç²å–å…§å­˜ä½¿ç”¨æƒ…æ³
            memory_usage = self.monitor.get_current_status().get('resources', {}).get('memory_used_gb', 0) * 1024
            
            # å‰µå»ºçµæœ
            result = OptimizedRetrievalResult(
                answer=answer,
                sources=sources,
                confidence=confidence,
                retrieval_time=retrieval_time,
                generation_time=generation_time,
                total_time=time.time() - start_time,
                metadata={
                    "query_enhanced": bool(memory_context),
                    "memory_context_length": len(memory_context),
                    "search_strategy": search_strategy,
                    "retrieval_metrics": retrieval_metrics.__dict__
                },
                cache_hit=False,
                retry_count=optimization_stats["retry_attempts"],
                batch_processed=use_batch_processing,
                memory_usage_mb=memory_usage,
                optimization_stats=optimization_stats
            )
            
            # ç·©å­˜çµæœ
            await self.cache.set(cache_key, result, ttl_seconds=3600)
            optimization_stats["cache_operations"] += 1
            
            # ä¿å­˜åˆ°è¨˜æ†¶ç³»çµ±
            if include_memory_context:
                await self._save_query_to_memory(question, result, memory_context)
            
            return result
            
        except Exception as e:
            optimization_stats["retry_attempts"] += 1
            # é‡æ–°æ‹‹å‡ºç•°å¸¸ï¼Œè®“é‡è©¦æ©Ÿåˆ¶è™•ç†
            raise e
    
    async def batch_query(self, 
                         questions: List[str],
                         **kwargs) -> List[OptimizedRetrievalResult]:
        """æ‰¹é‡æŸ¥è©¢"""
        
        # ä½¿ç”¨æ‰¹è™•ç†å™¨è™•ç†å¤šå€‹æŸ¥è©¢
        results = await self.batch_processor.process_batch(
            questions,
            lambda q: self.query(q, use_batch_processing=True, **kwargs)
        )
        
        return results.results
    
    async def parallel_query(self,
                           questions: List[str],
                           **kwargs) -> List[OptimizedRetrievalResult]:
        """ä¸¦è¡ŒæŸ¥è©¢"""
        
        # å‰µå»ºä¸¦è¡Œä»»å‹™
        tasks = []
        for i, question in enumerate(questions):
            task = Task(
                id=f"query_{i}",
                func=self.query,
                args=(question,),
                kwargs=kwargs
            )
            tasks.append(task)
        
        # åŸ·è¡Œä¸¦è¡Œä»»å‹™
        results = await self.parallel_executor.execute_parallel(tasks)
        
        return [r.result for r in results if r.success]
    
    @cached(ttl_seconds=1800, cache_level="memory")
    async def _generate_answer(self, question: str, sources: List[DocumentChunk]) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        
        if not sources:
            return "æŠ±æ­‰ï¼Œæˆ‘æ‰¾ä¸åˆ°ç›¸é—œçš„ä¿¡æ¯ä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚"
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, source in enumerate(sources[:3]):  # é™åˆ¶ä¸Šä¸‹æ–‡é•·åº¦
            context_parts.append(f"åƒè€ƒè³‡æ–™{i+1}ï¼š{source.content[:500]}...")
        
        context = "\n\n".join(context_parts)
        
        # æ§‹å»ºæç¤º
        prompt = f"""åŸºæ–¼ä»¥ä¸‹åƒè€ƒè³‡æ–™å›ç­”å•é¡Œï¼š

{context}

å•é¡Œï¼š{question}

è«‹æ ¹æ“šåƒè€ƒè³‡æ–™æä¾›æº–ç¢ºã€è©³ç´°çš„å›ç­”ã€‚å¦‚æœåƒè€ƒè³‡æ–™ä¸­æ²’æœ‰è¶³å¤ ä¿¡æ¯ï¼Œè«‹èªªæ˜ä¸¦æä¾›ä½ èƒ½ç¢ºå®šçš„éƒ¨åˆ†ä¿¡æ¯ã€‚

å›ç­”ï¼š"""
        
        try:
            response = await self.ollama_client.generate(
                model=self.settings.ollama.chat_model,
                prompt=prompt,
                temperature=self.settings.ollama.chat_temperature,
                max_tokens=self.settings.ollama.chat_max_tokens
            )
            
            return response.get('response', 'æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ™‚å‡ºç¾éŒ¯èª¤ã€‚')
            
        except Exception as e:
            return f"ç”Ÿæˆå›ç­”æ™‚å‡ºç¾éŒ¯èª¤ï¼š{str(e)}"
    
    async def _batch_generate_answer(self, question: str, sources: List[DocumentChunk]) -> str:
        """æ‰¹è™•ç†ç”Ÿæˆç­”æ¡ˆ"""
        
        # å°‡æºæ–‡æª”åˆ†æ‰¹è™•ç†
        batch_size = 3
        source_batches = [sources[i:i+batch_size] for i in range(0, len(sources), batch_size)]
        
        # ç‚ºæ¯å€‹æ‰¹æ¬¡ç”Ÿæˆéƒ¨åˆ†ç­”æ¡ˆ
        partial_answers = []
        for batch in source_batches:
            partial_answer = await self._generate_answer(question, batch)
            partial_answers.append(partial_answer)
        
        # åˆä½µéƒ¨åˆ†ç­”æ¡ˆ
        if len(partial_answers) == 1:
            return partial_answers[0]
        
        # ä½¿ç”¨LLMåˆä½µå¤šå€‹éƒ¨åˆ†ç­”æ¡ˆ
        combined_prompt = f"""è«‹å°‡ä»¥ä¸‹å¤šå€‹éƒ¨åˆ†ç­”æ¡ˆåˆä½µæˆä¸€å€‹å®Œæ•´ã€é€£è²«çš„å›ç­”ï¼š

å•é¡Œï¼š{question}

éƒ¨åˆ†ç­”æ¡ˆï¼š
{chr(10).join(f'{i+1}. {answer}' for i, answer in enumerate(partial_answers))}

è«‹æä¾›ä¸€å€‹åˆä½µå¾Œçš„å®Œæ•´ç­”æ¡ˆï¼š"""
        
        try:
            response = await self.ollama_client.generate(
                model=self.settings.ollama.chat_model,
                prompt=combined_prompt,
                temperature=0.3,  # è¼ƒä½æº«åº¦ç¢ºä¿ä¸€è‡´æ€§
                max_tokens=self.settings.ollama.chat_max_tokens
            )
            
            return response.get('response', partial_answers[0])
            
        except Exception:
            # å¦‚æœåˆä½µå¤±æ•—ï¼Œè¿”å›ç¬¬ä¸€å€‹éƒ¨åˆ†ç­”æ¡ˆ
            return partial_answers[0]
    
    def _calculate_confidence(self, search_results: List[SearchResult], answer: str) -> float:
        """è¨ˆç®—ç­”æ¡ˆç½®ä¿¡åº¦"""
        if not search_results:
            return 0.0
        
        # åŸºæ–¼æª¢ç´¢çµæœçš„å¹³å‡åˆ†æ•¸
        avg_score = sum(r.score for r in search_results) / len(search_results)
        
        # åŸºæ–¼ç­”æ¡ˆé•·åº¦çš„èª¿æ•´
        answer_length_factor = min(len(answer) / 200, 1.0)  # 200å­—ç¬¦ç‚ºåŸºæº–
        
        # åŸºæ–¼æºæ•¸é‡çš„èª¿æ•´
        source_factor = min(len(search_results) / 3, 1.0)  # 3å€‹æºç‚ºåŸºæº–
        
        # ç¶œåˆç½®ä¿¡åº¦
        confidence = avg_score * 0.6 + answer_length_factor * 0.2 + source_factor * 0.2
        
        return min(confidence, 1.0)
    
    async def _save_query_to_memory(self, question: str, result: OptimizedRetrievalResult, 
                                  memory_context: str):
        """ä¿å­˜æŸ¥è©¢åˆ°è¨˜æ†¶ç³»çµ±"""
        try:
            await self.context_manager.process_interaction(question, result.answer, {
                "confidence": result.confidence,
                "memory_context_used": bool(memory_context),
                "query_type": "optimized_rag_query",
                "optimization_stats": result.optimization_stats
            })
            
            # å¦‚æœç­”æ¡ˆè³ªé‡é«˜ï¼Œå­˜å„²åˆ°é•·æœŸè¨˜æ†¶
            if result.confidence > 0.8:
                from core.memory.base import MemoryImportance
                
                fact_content = f"å•é¡Œï¼š{question}\nç­”æ¡ˆï¼š{result.answer}"
                await self.context_manager.long_term_memory.store(
                    fact_content,
                    MemoryImportance.MEDIUM,
                    {
                        "source": "optimized_rag_query",
                        "confidence": result.confidence,
                        "query_timestamp": datetime.now().isoformat(),
                        "optimization_used": True
                    },
                    ["query", "knowledge", "optimized_rag"]
                )
                
        except Exception as e:
            print(f"ä¿å­˜æŸ¥è©¢åˆ°è¨˜æ†¶ç³»çµ±å¤±æ•—: {e}")
    
    async def ingest_documents(self, documents: List[Dict[str, Any]]):
        """å„ªåŒ–çš„æ–‡æª”æ”å…¥"""
        
        # ä½¿ç”¨æ‰¹è™•ç†æ”å…¥æ–‡æª”
        batch_result = await self.batch_processor.process_batch(
            documents,
            self._process_single_document
        )
        
        # æ·»åŠ åˆ°å¢å¼·æª¢ç´¢å™¨
        await self.enhanced_retriever.add_documents(documents)
        
        print(f"âœ… å·²æ”å…¥ {batch_result.items_processed} å€‹æ–‡æª”ï¼Œå¤±æ•— {batch_result.items_failed} å€‹")
        
        return batch_result
    
    async def _process_single_document(self, document: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†å–®å€‹æ–‡æª”"""
        # é€™è£¡å¯ä»¥æ·»åŠ æ–‡æª”é è™•ç†é‚è¼¯
        return {
            "id": document.get("id"),
            "processed": True,
            "content_length": len(document.get("content", ""))
        }
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """ç²å–å„ªåŒ–çµ±è¨ˆä¿¡æ¯"""
        return {
            "cache_stats": self.cache.get_stats(),
            "monitor_stats": self.monitor.get_current_status(),
            "retry_stats": self.retry_executor.get_statistics(),
            "retrieval_stats": self.enhanced_retriever.get_statistics(),
            "executor_stats": self.parallel_executor.get_execution_stats()
        }
    
    def optimize_system(self):
        """å„ªåŒ–ç³»çµ±æ€§èƒ½"""
        print("ğŸ”§ é–‹å§‹ç³»çµ±å„ªåŒ–...")
        
        # å„ªåŒ–æª¢ç´¢ç´¢å¼•
        self.enhanced_retriever.optimize_indices()
        
        # æ¸…ç†éæœŸç·©å­˜
        # é€™è£¡å¯ä»¥æ·»åŠ ç·©å­˜æ¸…ç†é‚è¼¯
        
        # ç²å–ç³»çµ±å»ºè­°
        recommendations = self.monitor.get_resource_recommendations()
        if recommendations:
            print("ğŸ’¡ ç³»çµ±å„ªåŒ–å»ºè­°:")
            for rec in recommendations:
                print(f"  - {rec}")
        
        print("âœ… ç³»çµ±å„ªåŒ–å®Œæˆ")
    
    def __del__(self):
        """æ¸…ç†è³‡æº"""
        try:
            if hasattr(self, 'parallel_executor'):
                self.parallel_executor.shutdown()
        except:
            pass


# å…¨å±€å„ªåŒ–RAGå¯¦ä¾‹
_global_optimized_rag = None


async def get_optimized_rag_system(user_id: str = "default") -> OptimizedRAGSystem:
    """ç²å–å„ªåŒ–çš„RAGç³»çµ±å¯¦ä¾‹"""
    global _global_optimized_rag
    if _global_optimized_rag is None:
        _global_optimized_rag = OptimizedRAGSystem(user_id)
        # å¯ä»¥åœ¨é€™è£¡æ·»åŠ åˆå§‹åŒ–é‚è¼¯
    return _global_optimized_rag


# ä¾¿æ·å‡½æ•¸
async def optimized_query(question: str, **kwargs) -> OptimizedRetrievalResult:
    """ä¾¿æ·çš„å„ªåŒ–æŸ¥è©¢å‡½æ•¸"""
    rag_system = await get_optimized_rag_system()
    return await rag_system.query(question, **kwargs)


async def batch_optimized_query(questions: List[str], **kwargs) -> List[OptimizedRetrievalResult]:
    """ä¾¿æ·çš„æ‰¹é‡å„ªåŒ–æŸ¥è©¢å‡½æ•¸"""
    rag_system = await get_optimized_rag_system()
    return await rag_system.batch_query(questions, **kwargs)
